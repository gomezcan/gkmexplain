{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the simulated data used in the DeepLIFT paper. The the pwms for the GATA motif and TAL motif (GATA_disc1 and TAL1_known1 from http://compbio.mit.edu/encode-motifs/) are used to generate motifs which are inserted into a random background. Sequences containing at least one GATA motif are a 1 for task 1, 0 otherwise. Sequences containing at least one TAL motif are a 1 for task 2, 0 otherwise. Sequences containing both a TAL and a GATA motif are a 1 for task 0. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File sequences.simdata.gz exists already\r\n"
     ]
    }
   ],
   "source": [
    "from __future__ import division, absolute_import, print_function\n",
    "!./grab_data.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    import simdna\n",
    "except ImportError, e:\n",
    "    print(\"installing simdna package\")\n",
    "    !pip install -e \"git://github.com/kundajelab/simdna.git@0.4.0#egg=simdna\"\n",
    "    print(\"\\n******************************************************************************\")\n",
    "    print(\"RESTART THE JUPYTER KERNEL TO PICK UP ON THE INSTALLATION!!!\")\n",
    "    print(\"******************************************************************************\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import simdna.synthetic as synthetic\n",
    "reload(synthetic)\n",
    "reload(synthetic.core)\n",
    "import gzip\n",
    "data_filename = \"sequences.simdata.gz\"\n",
    "data = synthetic.read_simdata_file(data_filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One-hot encode the sequence data into a 3d array, where the last axis (\"channel\" axis) is the ACGT axis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8000, 200, 4)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def one_hot_encode_along_channel_axis(sequence):\n",
    "    #theano dim ordering, uses row axis for one-hot\n",
    "    to_return = np.zeros((len(sequence),4), dtype=np.int8)\n",
    "    seq_to_one_hot_fill_in_array(zeros_array=to_return,\n",
    "                                 sequence=sequence, one_hot_axis=1)\n",
    "    return to_return\n",
    "\n",
    "def seq_to_one_hot_fill_in_array(zeros_array, sequence, one_hot_axis):\n",
    "    assert one_hot_axis==0 or one_hot_axis==1\n",
    "    if (one_hot_axis==0):\n",
    "        assert zeros_array.shape[1] == len(sequence)\n",
    "    elif (one_hot_axis==1): \n",
    "        assert zeros_array.shape[0] == len(sequence)\n",
    "    #will mutate zeros_array\n",
    "    for (i,char) in enumerate(sequence):\n",
    "        if (char==\"A\" or char==\"a\"):\n",
    "            char_idx = 0\n",
    "        elif (char==\"C\" or char==\"c\"):\n",
    "            char_idx = 1\n",
    "        elif (char==\"G\" or char==\"g\"):\n",
    "            char_idx = 2\n",
    "        elif (char==\"T\" or char==\"t\"):\n",
    "            char_idx = 3\n",
    "        elif (char==\"N\" or char==\"n\"):\n",
    "            continue #leave that pos as all 0's\n",
    "        else:\n",
    "            raise RuntimeError(\"Unsupported character: \"+str(char))\n",
    "        if (one_hot_axis==0):\n",
    "            zeros_array[char_idx,i] = 1\n",
    "        elif (one_hot_axis==1):\n",
    "            zeros_array[i,char_idx] = 1\n",
    "            \n",
    "onehot_data = np.array([one_hot_encode_along_channel_axis(seq) for seq in data.sequences])\n",
    "print(onehot_data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute the gapped kmer embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will compute the embeddings using the GPU to scan for matches to the gapped kmers, allowing for some numbed of mismatches. First, we will prepare the function that is going to do our scanning on the GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING (theano.sandbox.cuda): The cuda backend is deprecated and will be removed in the next release (v0.10).  Please switch to the gpuarray backend. You can get more information about how to switch at this URL:\n",
      " https://github.com/Theano/Theano/wiki/Converting-to-the-new-gpu-back-end%28gpuarray%29\n",
      "\n",
      "WARNING:theano.sandbox.cuda:The cuda backend is deprecated and will be removed in the next release (v0.10).  Please switch to the gpuarray backend. You can get more information about how to switch at this URL:\n",
      " https://github.com/Theano/Theano/wiki/Converting-to-the-new-gpu-back-end%28gpuarray%29\n",
      "\n",
      "Using gpu device 0: GeForce GT 750M (CNMeM is disabled, cuDNN 5005)\n"
     ]
    }
   ],
   "source": [
    "import ssvmimp\n",
    "import ssvmimp.train\n",
    "reload(ssvmimp.train)\n",
    "\n",
    "num_mismatches=1\n",
    "\n",
    "string_reps, embedding_func = ssvmimp.train.get_gapped_kmer_embedding_filters_and_func(\n",
    "                                kmer_len=6, alphabet=['A','C','G','T'],\n",
    "                                num_gaps=2, num_mismatches=num_mismatches)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "'string_reps' stores string representations of the filters, as shown below (gaps are indicated with whitespace):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of filters: 2560\n",
      "First ten filters:\n",
      "AAAA  \n",
      "AAAC  \n",
      "AAAG  \n",
      "AAAT  \n",
      "AACA  \n",
      "AACC  \n",
      "AACG  \n",
      "AACT  \n",
      "AAGA  \n",
      "AAGC  \n",
      "Last ten filters:\n",
      "T  TCG\n",
      "T  TCT\n",
      "T  TGA\n",
      "T  TGC\n",
      "T  TGG\n",
      "T  TGT\n",
      "T  TTA\n",
      "T  TTC\n",
      "T  TTG\n",
      "T  TTT\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of filters:\",len(string_reps))\n",
    "print(\"First ten filters:\")\n",
    "print(\"\\n\".join(string_reps[:10]))\n",
    "print(\"Last ten filters:\")\n",
    "print(\"\\n\".join(string_reps[-10:]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we compute the embeddings, which are the sum of the number of matches to each filter per sequence, accounting for the desired number of mismatches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done 0\n",
      "Done 500\n",
      "Done 1000\n",
      "Done 1500\n",
      "Done 2000\n",
      "Done 2500\n",
      "Done 3000\n",
      "Done 3500\n",
      "Done 4000\n",
      "Done 4500\n",
      "Done 5000\n",
      "Done 5500\n",
      "Done 6000\n",
      "Done 6500\n",
      "Done 7000\n",
      "Done 7500\n",
      "Shape of the embeddings matrix: (8000, 2560)\n"
     ]
    }
   ],
   "source": [
    "embeddings = embedding_func(onehot=onehot_data, batch_size=20, progress_update=500)\n",
    "print(\"Shape of the embeddings matrix:\",embeddings.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train SVMs given the embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train a linear classifier on task 1 (\"GATA present\") and task 2 (\"TAL present\"). Note that a classified trainer on task 0 (\"both TAL and GATA present\") doesn't perform very well because the concept of 'both' requires modeling logic that the linear SVM can't handle (I got around 50% accuracy when I tried it)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training t1\n",
      "Training t2\n"
     ]
    }
   ],
   "source": [
    "import sklearn.svm\n",
    "import sys\n",
    "\n",
    "train_set_num = 6000 #6000 examples will be used in the training set\n",
    "print(\"Training t1\")\n",
    "sys.stdout.flush()\n",
    "t1_classifier = sklearn.svm.LinearSVC().fit(X=embeddings[:train_set_num], y=data.labels[:train_set_num,1])\n",
    "print(\"Training t2\")\n",
    "sys.stdout.flush()\n",
    "t2_classifier = sklearn.svm.LinearSVC().fit(X=embeddings[:train_set_num], y=data.labels[:train_set_num,2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check the prediction accuracy on the testing set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for task 1 : 0.883\n",
      "Accuracy for task 2 : 0.922\n"
     ]
    }
   ],
   "source": [
    "for classifier, task in [(t1_classifier,1), (t2_classifier,2)]:\n",
    "    preds = classifier.predict(embeddings[train_set_num:])\n",
    "    print(\"Accuracy for task\",task,\":\",np.sum(data.labels[train_set_num:,task] == preds)/len(preds))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inspect the top-scoring filters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inspect the top-ranked filters for the two tasks by eye. coef_[0] stores the weights on the filters. As a reminder, the GATA PWM used was GATA_disc1 and the TAL motif used was TAL1_known1 from here: http://compbio.mit.edu/encode-motifs/. GATA is important for task 1 and TAL is important for task 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t1 filters:\n",
      "[(0.072894637318040925, 'C A TA'), (0.069944964948207497, 'GCT A '), (0.067494649630007258, 'T GAT '), (0.066725837772841551, 'G TCA '), (0.066143290731337498, 'G T AT'), (0.064245689025351826, 'TAT  G'), (0.063319230107679703, 'G TGA '), (0.05843796608403138, 'G TC G'), (0.058305746977908399, 'GC  AG'), (0.057219715270882515, 'TA AA ')]\n",
      "t2 filters:\n",
      "[(0.060981248398214422, 'C T TG'), (0.054347957653413922, 'C G GG'), (0.053985829750779327, 'AGTT  '), (0.052317984402812004, 'CA C G'), (0.049121796981378037, 'AT CT '), (0.048237652362894075, 'A CTG '), (0.048131761850582822, 'C GG G'), (0.047897503203327418, 'AA  AA'), (0.046779280342963442, 'TAT  T'), (0.045876630465575059, 'G AAA ')]\n"
     ]
    }
   ],
   "source": [
    "#get the top ranked filters for t1\n",
    "print(\"t1 filters:\")\n",
    "assert len(string_reps)==len(t1_classifier.coef_[0])\n",
    "t1_sorted_filters = sorted(zip(t1_classifier.coef_[0], string_reps), key=lambda x: -x[0])\n",
    "print(t1_sorted_filters[:10])\n",
    "\n",
    "print(\"t2 filters:\")\n",
    "assert len(string_reps)==len(t2_classifier.coef_[0])\n",
    "t2_sorted_filters = sorted(zip(t2_classifier.coef_[0], string_reps), key=lambda x: -x[0])\n",
    "print(t2_sorted_filters[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inspect the scores on specific kmers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The top ranked filters can be quite hard to make sense of. As a second sanity check, we look at the scores for the GATAAG kmer (strong match to the GATA motif) and the CAGATG kmer (strong match to the TAL motif) for task 0 and task 1. First, we define functions to help us look at the total score for a kmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from collections import OrderedDict\n",
    "\n",
    "def get_filter_matches(kmer_string, filter_strings, num_mismatches):\n",
    "    matching_filters = []\n",
    "    for filter_string in filter_strings:\n",
    "        match = True\n",
    "        mismatches_so_far = 0\n",
    "        for kmer_letter, filter_letter in zip(kmer_string, filter_string):\n",
    "            if (filter_letter!=\" \" and kmer_letter!=filter_letter):\n",
    "                mismatches_so_far += 1\n",
    "                if mismatches_so_far > num_mismatches:\n",
    "                    match=False\n",
    "                    break\n",
    "        if (match):\n",
    "            matching_filters.append(filter_string)\n",
    "    return matching_filters\n",
    "\n",
    "def get_total_kmer_score(kmer_string, filter_to_score, num_mismatches):\n",
    "    assert isinstance(filter_to_score, OrderedDict)\n",
    "    filter_matches = get_filter_matches(kmer_string=kmer_string, filter_strings=filter_to_score.keys(),\n",
    "                                        num_mismatches=num_mismatches)\n",
    "    total_score = 0\n",
    "    for a_filter in filter_matches:\n",
    "        total_score += filter_to_score[a_filter]\n",
    "    return total_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using these functions, we find that GATAAG scores pretty high for task 1 but not task 2, and CAGATG scores pretty high for task 2 but not task 1:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scores for GATAAG for task 1 and task 2:\n",
      "2.43074276237\n",
      "0.241003244462\n",
      "Scores for CAGATG for task 1 and task 2:\n",
      "0.384711815266\n",
      "2.02794532693\n"
     ]
    }
   ],
   "source": [
    "from collections import OrderedDict\n",
    "\n",
    "t1_filter_to_score = OrderedDict(zip(string_reps, t1_classifier.coef_[0]))\n",
    "t2_filter_to_score = OrderedDict(zip(string_reps, t2_classifier.coef_[0]))\n",
    "\n",
    "print(\"Scores for GATAAG for task 1 and task 2:\")\n",
    "print(get_total_kmer_score(kmer_string='GATAAG', filter_to_score=t1_filter_to_score, num_mismatches=num_mismatches))\n",
    "print(get_total_kmer_score(kmer_string='GATAAG', filter_to_score=t2_filter_to_score, num_mismatches=num_mismatches))\n",
    "print(\"Scores for CAGATG for task 1 and task 2:\")\n",
    "print(get_total_kmer_score(kmer_string='CAGATG', filter_to_score=t1_filter_to_score, num_mismatches=num_mismatches))\n",
    "print(get_total_kmer_score(kmer_string='CAGATG', filter_to_score=t2_filter_to_score, num_mismatches=num_mismatches))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
